"""
Engine - ä¸²è”ä¸€åˆ‡çš„ä¸»å¾ªç¯

è¿™æ˜¯ mini-vLLM çš„æ ¸å¿ƒå¼•æ“ï¼Œä¸²è” BlockManager å’Œ Scheduler
"""

from typing import List, Optional
from .block_manager import BlockManager
from .scheduler import Scheduler, Request


class InferenceEngine:
    """æ¨ç†å¼•æ“ä¸»å¾ªç¯"""
    
    def __init__(
        self,
        num_blocks: int = 100,
        block_size: int = 16,
        max_batch_size: int = 4
    ):
        """
        Args:
            num_blocks: æ˜¾å­˜æ€»å—æ•°
            block_size: æ¯å—å®¹é‡ï¼ˆtokensï¼‰
            max_batch_size: æœ€å¤§æ‰¹å¤„ç†å¤§å°
        """
        self.block_manager = BlockManager(num_blocks, block_size)
        self.scheduler = Scheduler(max_batch_size)
        self.iteration_count = 0
    
    def add_request(
        self,
        request_id: str,
        prompt: str,
        max_tokens: int = 100
    ):
        """æ·»åŠ æ¨ç†è¯·æ±‚"""
        request = Request(
            request_id=request_id,
            prompt=prompt,
            max_tokens=max_tokens
        )
        self.scheduler.add_request(request)
        print(f"âœ“ æ·»åŠ è¯·æ±‚: {request_id}, prompté•¿åº¦: {len(prompt)}")
    
    def step(self) -> bool:
        """
        æ‰§è¡Œä¸€ä¸ªæ¨ç†æ­¥éª¤
        
        Returns:
            æ˜¯å¦è¿˜æœ‰è¯·æ±‚åœ¨å¤„ç†
        """
        self.iteration_count += 1
        
        # 1. è°ƒåº¦ï¼šè·å–æœ¬æ¬¡è¦å¤„ç†çš„è¯·æ±‚
        batch = self.scheduler.schedule()
        
        if not batch:
            return False
        
        print(f"\n=== Iteration {self.iteration_count} ===")
        print(f"Batch size: {len(batch)}")
        print(f"Free blocks: {self.block_manager.get_num_free_blocks()}")
        
        # 2. ä¸ºæ¯ä¸ªè¯·æ±‚åˆ†é…æ˜¾å­˜ï¼ˆå¦‚æœéœ€è¦ï¼‰
        for req in batch:
            if req.request_id not in self.block_manager.allocated_blocks:
                try:
                    # ç®€åŒ–ï¼šå‡è®¾ prompt é•¿åº¦ä¸º 10 tokens
                    blocks = self.block_manager.allocate(req.request_id, num_tokens=10)
                    print(f"  â†’ {req.request_id}: åˆ†é…å— {blocks}")
                except RuntimeError as e:
                    print(f"  â†’ {req.request_id}: {e}")
                    continue
        
        # 3. æ¨¡æ‹Ÿæ¨ç†ï¼ˆç”Ÿæˆä¸€ä¸ª tokenï¼‰
        for req in batch:
            req.num_generated += 1
            print(f"  â†’ {req.request_id}: ç”Ÿæˆ token {req.num_generated}/{req.max_tokens}")
            
            # æ£€æŸ¥æ˜¯å¦å®Œæˆ
            if req.num_generated >= req.max_tokens:
                self.scheduler.mark_finished(req.request_id)
                self.block_manager.free(req.request_id)
                print(f"  âœ“ {req.request_id} å®Œæˆï¼é‡Šæ”¾æ˜¾å­˜")
        
        return True
    
    def run(self, max_iterations: int = 100):
        """è¿è¡Œä¸»å¾ªç¯ç›´åˆ°æ‰€æœ‰è¯·æ±‚å®Œæˆ"""
        print(f"\nğŸš€ å¯åŠ¨ mini-vLLM å¼•æ“")
        print(f"é…ç½®: {self.block_manager.num_blocks} å— Ã— {self.block_manager.block_size} tokens/å—")
        
        for _ in range(max_iterations):
            if not self.step():
                break
        
        print(f"\nâœ… å®Œæˆï¼æ€»å…±æ‰§è¡Œ {self.iteration_count} æ¬¡è¿­ä»£")

