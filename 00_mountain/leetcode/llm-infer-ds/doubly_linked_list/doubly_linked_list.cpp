// ==============================================================================
// Doubly Linked List - 对应 LLM Inference 中的 LRU Cache (显存置换)
// ==============================================================================
//
// 【对应引擎模块】
//   - LRU Cache: 显存块的热度管理和置换策略
//   - Block Eviction: 当显存满时，淘汰最久未使用的 KV block
//
// 【学习重点】
//   1. 显存块热度维护：最近访问的块移到链表头部
//   2. 节点插入删除的指针操作：前驱/后继指针的正确维护
//   3. O(1) 的移动操作：结合哈希表实现快速定位+移动
//
// 【实现要求】
//   - 实现双向链表的基本操作：insert, remove, move_to_front
//   - 每个节点包含 prev 和 next 指针
//   - 维护 head 和 tail 哨兵节点，简化边界条件处理
//   - 配合哈希表实现 LRU Cache（见 LC 146）
//
// 【练习目标】
//   - 深入理解指针操作，避免野指针和内存泄漏
//   - 掌握 vLLM 中显存块置换的 LRU 策略
//   - 练习复杂的指针链接操作（4指针修改：node.prev, node.next, prev.next, next.prev）
// ==============================================================================

