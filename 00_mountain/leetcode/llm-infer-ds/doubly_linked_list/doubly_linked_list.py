# ==============================================================================
# Doubly Linked List - 对应 LLM Inference 中的 LRU Cache (显存置换)
# ==============================================================================
#
# 【对应引擎模块】
#   - LRU Cache: 显存块的热度管理和置换策略
#   - Block Eviction: 当显存满时，淘汰最久未使用的 KV block
#
# 【学习重点】
#   1. 显存块热度维护：最近访问的块移到链表头部
#   2. 节点插入删除的指针操作：前驱/后继指针的正确维护
#   3. O(1) 的移动操作：结合哈希表实现快速定位+移动
#
# 【实现要求】
#   - 实现 Node 类：包含 value, prev, next 属性
#   - 实现双向链表的基本操作：insert, remove, move_to_front
#   - 维护 head 和 tail 哨兵节点，简化边界条件处理
#   - 配合 dict 实现 LRU Cache（见 LC 146）
#
# 【练习目标】
#   - 理解 Python 中对象引用和垃圾回收
#   - 掌握 vLLM 中显存块置换的 LRU 策略
#   - 练习复杂的指针链接操作（虽然 Python 用引用）
#
# 【提示】
#   - Python 无需手动释放内存，GC 会自动回收
#   - 建议使用哨兵节点简化边界条件
#   - 可以实现 __iter__ 方法支持遍历
# ==============================================================================

