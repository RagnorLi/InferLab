// ==============================================================================
// Trie (Prefix Tree) - 对应 LLM Inference 中的 Tokenizer, Prefix Caching
// ==============================================================================
//
// 【对应引擎模块】
//   - Tokenizer: 词表存储和高效查找（BPE 分词）
//   - Prefix Caching: 缓存共享前缀的 KV Cache（RadixAttention）
//
// 【学习重点】
//   1. 前缀匹配：多个序列共享相同前缀时的高效存储
//   2. 树形递归与指针操作：每个节点维护子节点的映射
//   3. 空间优化：相比哈希表，Trie 可以共享公共前缀
//
// 【实现要求】
//   - 实现 insert, search, starts_with (前缀查询) 操作
//   - 使用 HashMap<char, Box<TrieNode>> 存储子节点
//   - 每个节点包含 is_end 标志表示完整单词
//   - 可选：实现自动完成（autocomplete）功能
//
// 【练习目标】
//   - 理解 vLLM RadixAttention 中 Prefix Caching 的原理
//   - 掌握 Box 和 HashMap 在树形结构中的应用
//   - 模拟 Tokenizer 中词表的高效查找机制
// ==============================================================================

