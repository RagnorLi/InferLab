---
name: LLM推理架构师：世界顶级路径
overview: 8-9个月，8个Phase，从TinyLlama到DeepSeek V3，从GQA到MLA，从INT4到FP8，从Dense Model到MoE。目标：不是"会用vLLM"，而是"能设计下一代vLLM"。达到Principal/Architect级能力（年薪150万+）。
todos:
  - id: module0
    content: "Module 0: 性能分析工具链"
    status: pending
  - id: phase0
    content: "Phase 0: TinyLlama基础 + CUDA基础"
    status: pending
    dependencies:
      - module0
  - id: phase1
    content: "Phase 1: Rust内存管理器"
    status: pending
    dependencies:
      - phase0
  - id: phase2
    content: "Phase 2: DeepSeek架构专项 (RoPE + MLA)"
    status: pending
    dependencies:
      - phase1
  - id: phase3
    content: "Phase 3: Paged Attention + FP8 KV Cache"
    status: pending
    dependencies:
      - phase2
  - id: phase4
    content: "Phase 4: 调度器 + HTTP服务"
    status: pending
    dependencies:
      - phase3
  - id: phase5
    content: "Phase 5: FP8推理 + INT4混合量化"
    status: pending
    dependencies:
      - phase4
  - id: phase6
    content: "Phase 6: MoE架构 + Expert Parallelism"
    status: pending
    dependencies:
      - phase5
  - id: phase7
    content: "Phase 7: Speculative Decoding + Medusa"
    status: pending
    dependencies:
      - phase6
  - id: phase8
    content: "Phase 8: 长上下文 + RadixAttention"
    status: pending
    dependencies:
      - phase7
---

# LLM推理架构师：世界顶级路径（终极版）

## 评审者的警告：你在和谁竞争？

当你6-9个月后去面试DeepSeek、OpenAI、Anthropic时，你的竞争对手是：

- **CMU/Stanford的PhD**：发过顶会论文（FlashAttention、Medusa的作者）
- **Meta/Google的Staff Engineer**：写过PyTorch/JAX的核心代码
- **vLLM/SGLang的核心贡献者**：有上千stars的开源项目

**你的优势**：他们懂理论，但你要**懂工业界的下一个痛点**。---

## 核心升级：从2023到2025

### 对比：原计划 vs 终极版本

| 维度 | 原计划 (90分) | 终极版本 (120分) ||------|--------------|----------------|| **模型架构** | Dense Model (TinyLlama) | Dense + MoE (DeepSeek V3) || **Attention** | GQA + PagedAttention | GQA + MLA + PagedAttention || **量化** | INT4 (AWQ) | INT4 + FP8 (含KV Cache量化) || **分布式** | Tensor Parallelism | TP + Expert Parallelism (EP) || **延迟优化** | 无 | Speculative Decoding + Medusa || **长上下文** | 无 | RadixAttention (Prefix Caching) || **目标级别** | Senior/Staff (80万) | Principal/Architect (150万+) |---

## Module 0: 性能分析工具链（Week 0）

**【这部分保持不变】**详见之前的计划。核心：

- Roofline Model
- Nsight Systems + Compute
- Profiling Report模板

---

## Phase 0: TinyLlama基础（Week 1-3）

**【这部分保持不变】**目标：

- 100行Python跑通TinyLlama
- 3个CUDA Kernel（vector add, naive matmul, tiled matmul）
- 加入KV Cache，速度提升10倍

**为什么不直接上MoE？**

- 你需要先理解Dense Model的推理流程
- CUDA基础必须打牢
- KV Cache的概念是后续所有优化的基础

---

## Phase 1: Rust内存管理器（Week 4-6）

**【这部分保持不变】**目标：

- Rust实现PagedAttention的BlockAllocator
- Buddy Allocator优化
- PyO3绑定到Python

---

## Phase 2: DeepSeek架构专项（Week 7-10）⚡ 升级

> **评审者的核心建议**：不要只做RoPE，要理解DeepSeek V2/V3的MLA架构

### 2.1 传统RoPE实现（Week 7）

**【保留原计划】**：手写RoPE CUDA Kernel

### 2.2 MLA核心原理（Week 8-9）⚡ 新增

**什么是MLA？**Multi-Head Latent Attention是DeepSeek V2/V3的核心创新：

```javascript
传统GQA (Llama-3):
- Q: (batch, seq, num_heads=32, head_dim=128) -> 4096维
- K: (batch, seq, num_kv_heads=8, head_dim=128) -> 1024维
- V: (batch, seq, num_kv_heads=8, head_dim=128) -> 1024维
→ KV Cache: 2048维/token

MLA (DeepSeek V3):
- Q: 4096维（不变）
- KV压缩: 先投影到低维Latent Space (512维)
- K_compressed: (batch, seq, latent_dim=512)
- V_compressed: (batch, seq, latent_dim=512)
→ KV Cache: 1024维/token （减少50%！）

关键：在计算Attention时，动态地把compressed KV解压回高维
```

**核心挑战**：

1. **Matrix Absorption**：把解压的投影矩阵"吸收"到后续的计算中
2. **RoPE的解耦**：MLA对Q和压缩的K都做RoPE，但维度不同

**交付物**：[`mountain/hpc/llm/mla_attention.py`](mountain/hpc/llm/mla_attention.py)**【框架代码】**：朴素的MLA实现

```python
class MLAAttention:
    def __init__(self, d_model=4096, latent_dim=512, num_heads=32):
        # 压缩投影
        self.kv_compress = nn.Linear(d_model, latent_dim)
        # 解压投影
        self.k_decompress = nn.Linear(latent_dim, num_heads * head_dim)
        self.v_decompress = nn.Linear(latent_dim, num_heads * head_dim)
        # Q投影
        self.q_proj = nn.Linear(d_model, num_heads * head_dim)
    
    def forward(self, x, kv_cache_compressed):
        # 压缩KV
        kv_compressed = self.kv_compress(x)  # (batch, seq, 512)
        
        # 添加到KV Cache（只存512维！）
        kv_cache_compressed = torch.cat([kv_cache_compressed, kv_compressed], dim=1)
        
        # 解压K, V
        k = self.k_decompress(kv_cache_compressed)  # (batch, full_seq, 4096)
        v = self.v_decompress(kv_cache_compressed)
        
        # 计算Q
        q = self.q_proj(x[:, -1:])  # 只计算最后一个token
        
        # Attention
        attn_out = scaled_dot_product_attention(q, k, v)
        
        return attn_out, kv_cache_compressed
```

**【你的优化任务】**：

1. **实现Matrix Absorption优化**
   ```python
                                          # TODO: 把k_decompress和o_proj合并
                                          # 原来：x -> KV_compress -> K_decompress -> Attention -> O_proj
                                          # 优化：x -> KV_compress -> Attention (用融合的权重)
                                          
                                          # 目标：减少一次矩阵乘法
   ```




2. **Triton实现MLA Paged Attention**
   ```python
                                          # TODO: 修改Phase 3的PagedAttention
                                          # 挑战：KV Cache现在是compressed的
                                          # 需要在Kernel中做解压
                                          
                                          @triton.jit
                                          def mla_paged_attention(
                                              Q, 
                                              KV_cache_compressed,  # (num_blocks, block_size, latent_dim)
                                              K_decompress_weight,  # (latent_dim, hidden_dim)
                                              V_decompress_weight,
                                              block_table,
                                              ...
                                          ):
                                              # 从物理块读取compressed KV
                                              kv_compressed = tl.load(KV_cache_compressed[physical_block])
                                              
                                              # 在Shared Memory中解压
                                              k = kv_compressed @ K_decompress_weight
                                              v = kv_compressed @ V_decompress_weight
                                              
                                              # 计算Attention
                                              ...
   ```




3. **Benchmark：MLA vs GQA的显存占用**
   ```python
                                          # TODO: 对比长文本场景（128K tokens）
                                          # GQA: 128K * 2048维 = 262MB
                                          # MLA: 128K * 1024维 = 131MB （减少50%）
                                          
                                          # 这就是为什么DeepSeek V3能支持128K上下文
   ```


**验收标准**：

- ✅ 实现MLA的完整前向传播
- ✅ KV Cache显存占用减少50%
- ✅ 性能不低于GQA（解压的开销被显存节省抵消）
- ✅ 理解：为什么MLA是"长上下文"的关键

**【论文阅读】**：

- DeepSeek V2论文：MLA部分
- DeepSeek V3技术报告
- 写博客：《MLA：DeepSeek V3如何把KV Cache压缩50%》

---

## Phase 3: Paged Attention + FP8 KV Cache（Week 11-13）⚡ 增强

### 3.1 PagedAttention（Week 11）

**【保留原计划】**：Triton实现Paged Attention

### 3.2 FP8 KV Cache（Week 12-13）⚡ 新增

> **评审者的核心建议**：长上下文场景下，KV Cache占用的显存可能比模型权重还大**什么是FP8？**

```javascript
FP16: 1 sign + 5 exponent + 10 mantissa = 16 bits
FP8 (E4M3): 1 sign + 4 exponent + 3 mantissa = 8 bits
FP8 (E5M2): 1 sign + 5 exponent + 2 mantissa = 8 bits

挑战：动态范围很小，需要Per-Tensor或Per-Token的Scale
```

**交付物**：[`mountain/hpc/triton/fp8_kv_cache.py`](mountain/hpc/triton/fp8_kv_cache.py)**【框架代码】**：朴素的FP8量化

```python
def quantize_fp8(tensor, scale):
    """FP16 -> FP8"""
    return (tensor / scale).to(torch.float8_e4m3fn)

def dequantize_fp8(tensor_fp8, scale):
    """FP8 -> FP16"""
    return tensor_fp8.to(torch.float16) * scale
```

**【你的优化任务】**：

1. **动态Scale计算**
   ```python
                                          # TODO: Per-Token的Scale
                                          # 每个token的KV有不同的分布
                                          # 需要计算每个token的max值，作为scale
                                          
                                          def compute_scale_per_token(kv_cache):
                                              # kv_cache: (batch, seq_len, hidden_dim)
                                              max_vals = kv_cache.abs().max(dim=-1, keepdim=True)[0]
                                              scale = max_vals / 448.0  # FP8 E4M3的最大值
                                              return scale
   ```




2. **Triton实现FP8 Paged Attention**
   ```python
                                          # TODO: 在PagedAttention Kernel中
                                          # KV Cache以FP8格式存储
                                          # 读取时动态dequantize
                                          
                                          @triton.jit
                                          def fp8_paged_attention(
                                              Q_fp16,
                                              KV_cache_fp8,     # 存储为FP8
                                              KV_scales,        # Per-Token的Scale
                                              block_table,
                                              ...
                                          ):
                                              # 从物理块读取FP8 KV
                                              kv_fp8 = tl.load(KV_cache_fp8[physical_block])
                                              scale = tl.load(KV_scales[physical_block])
                                              
                                              # Dequantize到FP16
                                              kv_fp16 = kv_fp8.to(tl.float16) * scale
                                              
                                              # 计算Attention
                                              ...
   ```




3. **Benchmark：FP8 vs FP16的精度和速度**
   ```python
                                          # TODO: 对比128K上下文
                                          # FP16 KV Cache: 262MB
                                          # FP8 KV Cache: 131MB（减少50%）
                                          
                                          # 精度损失：<1%（在长文本任务上测试）
                                          # 速度：可能稍慢（因为dequantize），但显存节省允许更大batch
   ```


**验收标准**：

- ✅ FP8 KV Cache显存减少50%
- ✅ 精度损失<1%（在长文本QA任务上）
- ✅ 支持128K上下文（FP16只能支持64K）
- ✅ 理解：为什么FP8是"长上下文"的标配

**【论文阅读】**：

- FP8 Formats for Deep Learning (NVIDIA)
- Atom: Low-bit Quantization for Efficient and Accurate LLM Serving
- 写博客：《FP8 KV Cache：让Llama-3支持1M上下文》

---

## Phase 4: 调度器（Week 14-16）

**【保留原计划】**目标：

- Continuous Batching
- FCFS + Priority + Preemption
- FastAPI HTTP服务
- GPU利用率>80%

---

## Phase 5: FP8推理 + INT4混合量化（Week 17-20）⚡ 升级

> **评审者的核心建议**：INT4虽好，但FP8是H100/B200的核心战力

### 5.1 INT4量化（Week 17）

**【保留原计划】**：AWQ量化TinyLlama

### 5.2 FP8 GEMM（Week 18-19）⚡ 新增

**为什么需要FP8？**H100的FP8 Tensor Core吞吐量是FP16的2倍：

- FP16: 989 TFLOPS
- FP8: 1979 TFLOPS

**交付物**：[`mountain/hpc/triton/fp8_gemm.py`](mountain/hpc/triton/fp8_gemm.py)**【你的优化任务】**：

1. **Triton实现FP8 GEMM**
   ```python
                                          @triton.jit
                                          def fp8_matmul_kernel(
                                              A_fp8, B_fp8,        # 输入：FP8
                                              C_fp16,              # 输出：FP16
                                              scale_a, scale_b,    # Scale
                                              M, N, K,
                                              BLOCK_M: tl.constexpr,
                                              BLOCK_N: tl.constexpr,
                                              BLOCK_K: tl.constexpr,
                                          ):
                                              # 加载FP8数据
                                              a_fp8 = tl.load(A_fp8 + offsets)
                                              b_fp8 = tl.load(B_fp8 + offsets)
                                              
                                              # Cast到FP16（Triton会自动调用Tensor Core）
                                              a_fp16 = a_fp8.to(tl.float16) * scale_a
                                              b_fp16 = b_fp8.to(tl.float16) * scale_b
                                              
                                              # 矩阵乘法
                                              c = tl.dot(a_fp16, b_fp16)
                                              
                                              tl.store(C_fp16 + offsets, c)
   ```




2. **对比cuBLAS的FP8实现**
   ```python
                                          # TODO: 调用NVIDIA的cublasLtMatmul（FP8版本）
                                          # 对比你的Triton实现 vs cuBLAS
                                          # 目标：达到cuBLAS的80%性能
   ```




3. **混合精度策略**
   ```python
                                          # TODO: 权重用FP8，激活值用FP16
                                          # 在DeepSeek V3中：
                                          # - MLP权重：FP8（不敏感）
                                          # - Attention权重：FP16（敏感）
                                          # - KV Cache：FP8（Phase 3已实现）
                                          
                                          # 找到最优的混合策略
   ```


**验收标准**：

- ✅ FP8 GEMM性能达到cuBLAS的80%
- ✅ 端到端推理速度提升1.5倍（相比FP16）
- ✅ 精度损失<1.5%
- ✅ 理解：为什么FP8比INT4精度高？（因为有指数位）

### 5.3 量化对比分析（Week 20）

**任务**：在5个benchmark上对比：| 方法 | 显存 | 速度 | 精度 | 适用场景 ||------|------|------|------|---------|| FP16 | 1x | 1x | 100% | 基准 || INT4 (AWQ) | 0.25x | 1.8x | 98% | 显存极度受限 || FP8 | 0.5x | 1.5x | 99.5% | H100/B200标配 || FP8权重+FP8 KV | 0.5x | 1.6x | 99% | 长上下文 |**【论文阅读】**：

- FP8 Formats for Deep Learning (NVIDIA)
- LLM.int8(): 8-bit Matrix Multiplication
- 写博客：《FP8 vs INT4：LLM量化的终极对决》

---

## Phase 6: MoE架构 + Expert Parallelism（Week 21-24）⚡ 升级

> **评审者的核心建议**：不懂MoE，就是在玩"小模型"

### 6.1 理解MoE架构（Week 21）

**什么是MoE？**

```javascript
Dense Model (Llama-3-8B):
- 每个token都过所有参数（8B）
- 计算量：8B FLOPs/token

MoE (DeepSeek V3):
- 总参数：671B
- 激活参数：37B（每个token只用一小部分专家）
- 有256个专家，每次选Top-K=8个专家
- 计算量：37B FLOPs/token（比Dense的70B还少！）
```

**核心挑战**：

1. **Routing**：如何高效地选出Top-K专家？
2. **Load Balancing**：如何避免所有token都选同一个专家？
3. **Expert Parallelism**：当GPU数>专家数时，如何调度？

**交付物**：[`mountain/hpc/llm/moe_layer.py`](mountain/hpc/llm/moe_layer.py)**【框架代码】**：朴素的MoE实现

```python
class MoELayer:
    def __init__(self, num_experts=8, top_k=2, hidden_dim=4096):
        self.router = nn.Linear(hidden_dim, num_experts)
        self.experts = nn.ModuleList([
            nn.Linear(hidden_dim, hidden_dim) for _ in range(num_experts)
        ])
        self.top_k = top_k
    
    def forward(self, x):
        # x: (batch, seq, hidden_dim)
        
        # 1. Routing
        router_logits = self.router(x)  # (batch, seq, num_experts)
        router_probs = F.softmax(router_logits, dim=-1)
        
        # 2. Top-K选择
        topk_probs, topk_indices = torch.topk(router_probs, self.top_k, dim=-1)
        # topk_indices: (batch, seq, top_k)
        
        # 3. 分发到专家（朴素实现：循环）
        output = torch.zeros_like(x)
        for batch_idx in range(x.shape[0]):
            for seq_idx in range(x.shape[1]):
                for k_idx in range(self.top_k):
                    expert_id = topk_indices[batch_idx, seq_idx, k_idx]
                    prob = topk_probs[batch_idx, seq_idx, k_idx]
                    expert_out = self.experts[expert_id](x[batch_idx, seq_idx])
                    output[batch_idx, seq_idx] += prob * expert_out
        
        return output
```

**【你的优化任务】**：

### 6.2 高效的Expert Routing Kernel（Week 22）⚡ 新增

1. **Triton实现批量Routing**
   ```python
                                          # TODO: 把token按专家ID分组
                                          # 挑战：不同token可能选不同的专家
                                          # 需要动态构建expert_to_tokens的映射
                                          
                                          @triton.jit
                                          def moe_routing_kernel(
                                              tokens,           # (total_tokens, hidden_dim)
                                              router_weights,   # (hidden_dim, num_experts)
                                              topk_indices,     # 输出: (total_tokens, top_k)
                                              topk_probs,       # 输出: (total_tokens, top_k)
                                              num_experts, top_k,
                                          ):
                                              # 计算router_logits
                                              # Top-K选择（用Triton的sort）
                                              # 输出topk_indices和topk_probs
                                              ...
   ```




2. **Expert Batching优化**
   ```python
                                          # TODO: 把选中同一个专家的token打包成batch
                                          # 原来：每个token单独过专家（慢）
                                          # 优化：把100个选expert_0的token打包，一次性计算
                                          
                                          def expert_batching(x, topk_indices):
                                              # 构建expert_to_tokens映射
                                              expert_to_tokens = {}
                                              for token_id, expert_ids in enumerate(topk_indices):
                                                  for expert_id in expert_ids:
                                                      if expert_id not in expert_to_tokens:
                                                          expert_to_tokens[expert_id] = []
                                                      expert_to_tokens[expert_id].append(token_id)
                                              
                                              # 批量计算
                                              outputs = []
                                              for expert_id, token_ids in expert_to_tokens.items():
                                                  batch_input = x[token_ids]  # 打包
                                                  batch_output = self.experts[expert_id](batch_input)
                                                  outputs.append((token_ids, batch_output))
                                              
                                              # 重组输出
                                              ...
   ```




### 6.3 Expert Parallelism（Week 23-24）⚡ 新增

**核心挑战**：当你有8个GPU，但模型有8个专家时，如何分布？**三种策略**：

```javascript
策略1：Expert Parallelism (EP)
- 每个GPU放1个专家
- 问题：负载不均衡（某些专家被选中次数多）

策略2：EP + TP混合
- 每个GPU放1个专家，但专家内部用TP切分
- DeepSeek V3的做法

策略3：Replication
- 热门专家在多个GPU上复制
- 冷门专家只在1个GPU上
```

**【你的优化任务】**：

1. **实现EP + TP混合**
   ```python
                                          # TODO: 
                                          # - Expert间：用Expert Parallelism（All-to-All通信）
                                          # - Expert内：用Tensor Parallelism（All-Reduce通信）
                                          
                                          class DistributedMoELayer:
                                              def __init__(self, rank, world_size, num_experts_per_gpu):
                                                  # 每个GPU负责 num_experts_per_gpu 个专家
                                                  self.local_experts = [
                                                      DistributedExpert(rank, world_size)  # 内部用TP
                                                      for _ in range(num_experts_per_gpu)
                                                  ]
                                              
                                              def forward(self, x):
                                                  # 1. All-to-All: 把token路由到对应的GPU
                                                  x_routed = all_to_all_dispatch(x, topk_indices)
                                                  
                                                  # 2. 本地计算（Expert内部用TP）
                                                  local_output = compute_local_experts(x_routed)
                                                  
                                                  # 3. All-to-All: 把结果发回原GPU
                                                  output = all_to_all_combine(local_output)
                                                  
                                                  return output
   ```




2. **Benchmark：通信开销分析**
   ```python
                                          # TODO: 用Nsight Systems分析
                                          # - All-to-All通信占总时间的百分比
                                          # - 是否有load imbalance（某些GPU空闲）
                                          
                                          # DeepSeek V3的数据：All-to-All占20-30%
                                          # 你的目标：控制在30%以内
   ```




3. **Load Balancing策略**
   ```python
                                          # TODO: 实现Auxiliary Loss
                                          # 目标：让token均匀分布到各个专家
                                          
                                          def load_balancing_loss(router_probs, expert_counts):
                                              # router_probs: (total_tokens, num_experts)
                                              # expert_counts: 每个专家被选中的次数
                                              
                                              # 计算期望分布（均匀）vs 实际分布
                                              target_ratio = 1.0 / num_experts
                                              actual_ratio = expert_counts / expert_counts.sum()
                                              
                                              # 惩罚不均衡
                                              loss = ((actual_ratio - target_ratio) ** 2).sum()
                                              return loss
   ```


**验收标准**：

- ✅ 实现MoE的完整前向传播
- ✅ Expert Batching优化，吞吐量提升2-3倍
- ✅ EP + TP混合，8卡吞吐量提升6倍以上
- ✅ All-to-All通信开销<30%
- ✅ 理解：为什么MoE是"scaling law"的关键

**【论文阅读】**：

- Switch Transformers (Google)
- DeepSeek V2/V3技术报告（MoE部分）
- GShard: Scaling Giant Models with Conditional Computation
- 写博客：《DeepSeek V3的MoE架构：256个专家如何高效调度》

---

## Phase 7: Speculative Decoding + Medusa（Week 25-27）⚡ 新增

> **评审者的核心建议**：Throughput是成本，Latency是用户体验

### 7.1 理解Speculative Decoding（Week 25）

**核心思想**：

```javascript
传统Decoding（串行）：
Token 1 → Token 2 → Token 3 → ...
每个token需要1次Forward Pass

Speculative Decoding（并行验证）：
Draft Model快速生成5个token：[T1, T2, T3, T4, T5]
Target Model并行验证这5个token
如果前3个正确：接受，节省了2次Forward Pass
如果第2个错误：拒绝后面的，重新生成

加速比：2-5x（取决于Draft Model的准确率）
```

**交付物**：[`mountain/hpc/llm/speculative_decoding.py`](mountain/hpc/llm/speculative_decoding.py)**【框架代码】**：基础的Speculative Decoding

```python
class SpeculativeDecoding:
    def __init__(self, draft_model, target_model):
        self.draft = draft_model  # 小模型（如500M）
        self.target = target_model  # 大模型（如8B）
    
    def generate(self, prompt, num_tokens, gamma=5):
        tokens = encode(prompt)
        
        while len(tokens) < num_tokens:
            # 1. Draft：小模型生成gamma个token
            draft_tokens = []
            draft_probs = []
            for _ in range(gamma):
                logits = self.draft.forward(tokens + draft_tokens)
                prob = softmax(logits)
                next_token = sample(prob)
                draft_tokens.append(next_token)
                draft_probs.append(prob[next_token])
            
            # 2. Verify：大模型并行验证
            # 关键：一次Forward Pass验证gamma个token
            all_tokens = tokens + draft_tokens
            target_logits = self.target.forward(all_tokens)
            target_probs = softmax(target_logits)
            
            # 3. Accept/Reject
            accepted = []
            for i in range(gamma):
                target_prob = target_probs[len(tokens) + i, draft_tokens[i]]
                draft_prob = draft_probs[i]
                
                # Rejection Sampling
                if random.random() < min(1, target_prob / draft_prob):
                    accepted.append(draft_tokens[i])
                else:
                    break  # 拒绝后面的
            
            tokens += accepted
            
            # 如果全部拒绝，用Target Model生成1个token
            if len(accepted) == 0:
                next_token = sample(target_probs[len(tokens)])
                tokens.append(next_token)
        
        return tokens
```

**【你的优化任务】**：

1. **Tree Attention优化**
   ```python
                                          # TODO: 在验证阶段，不是验证线性序列，而是验证一棵Token树
                                          # 例如：第2个token有3个候选，每个候选后面有2个候选
                                          # 需要构建特殊的Attention Mask
                                          
                                          def build_tree_attention_mask(draft_tree):
                                              # draft_tree: {
                                              #   0: [1, 2],      # token 0后面可能是1或2
                                              #   1: [3, 4],      # token 1后面可能是3或4
                                              #   2: [5],         # token 2后面是5
                                              # }
                                              
                                              # 构建Attention Mask，让每个候选只能看到它的祖先
                                              mask = build_mask(draft_tree)
                                              
                                              # 一次Forward Pass验证整棵树
                                              logits = target_model.forward(all_candidates, attention_mask=mask)
                                              
                                              # 用Best-First Search选择最优路径
                                              best_path = bfs(draft_tree, logits)
                                              return best_path
   ```




### 7.2 Medusa架构（Week 26-27）⚡ 新增

**Medusa的创新**：不需要Draft Model，而是给Target Model加多个"头"

```javascript
传统：1个LM Head → 生成1个token

Medusa：N个LM Head → 并行生成N个token
Head 1: 预测token 1
Head 2: 预测token 2（基于token 1的embedding）
Head 3: 预测token 3（基于token 1+2的embedding）
...

然后用Tree Attention验证这N个token
```

**【你的优化任务】**：

1. **训练Medusa Heads**
   ```python
                                          # TODO: 给TinyLlama加3个额外的LM Head
                                          # 训练数据：让Head i学习预测第i个future token
                                          
                                          class MedusaModel:
                                              def __init__(self, base_model, num_heads=3):
                                                  self.base = base_model
                                                  self.medusa_heads = nn.ModuleList([
                                                      nn.Linear(hidden_dim, vocab_size)
                                                      for _ in range(num_heads)
                                                  ])
                                              
                                              def forward(self, tokens):
                                                  hidden = self.base(tokens)
                                                  
                                                  # 每个head预测不同位置的token
                                                  predictions = []
                                                  for i, head in enumerate(self.medusa_heads):
                                                      pred = head(hidden[:, -(i+1)])
                                                      predictions.append(pred)
                                                  
                                                  return predictions
   ```




2. **Medusa的Tree Attention**
   ```python
                                          # TODO: 构建候选树
                                          # Head 1: Top-K=10个候选
                                          # Head 2: 对每个Head 1的候选，生成Top-K=3个候选
                                          # Head 3: 对每个Head 2的候选，生成Top-K=2个候选
                                          # 总共：10 * 3 * 2 = 60个候选路径
                                          
                                          # 用Tree Attention一次性验证60个候选
                                          # 选择概率最高的路径
   ```


**验收标准**：

- ✅ Speculative Decoding加速2-3倍
- ✅ Medusa加速1.5-2倍（无需Draft Model）
- ✅ 理解：为什么Speculative Decoding有时比Medusa快？（Draft Model很小时）
- ✅ 用Nsight Systems分析：验证阶段的并行度

**【论文阅读】**：

- Fast Inference from Transformers via Speculative Decoding
- Medusa: Simple LLM Inference Acceleration with Multiple Decoding Heads
- EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty
- 写博客：《Speculative Decoding：让Llama推理快3倍的黑科技》

---

## Phase 8: 长上下文 + RadixAttention（Week 28-30）⚡ 新增

> **评审者暗示**：长上下文是2025年的核心战场

### 8.1 长上下文的挑战

**问题**：

- 128K上下文，KV Cache需要几十GB显存
- 多轮对话时，大部分上下文是重复的（system prompt、历史对话）
- 每次都重算浪费算力

**解决方案**：RadixAttention (Prefix Caching)

```javascript
场景：多用户chat with same system prompt
User 1: "你好，帮我..."
User 2: "你好，帮我..."
User 3: "你好，帮我..."

传统：每个用户都计算一次system prompt的KV Cache
RadixAttention：system prompt的KV Cache共享！
```

**交付物**：[`mountain/hpc/llm/radix_attention.py`](mountain/hpc/llm/radix_attention.py)**【框架代码】**：基础的Prefix Caching

```python
class PrefixCache:
    def __init__(self):
        self.cache = {}  # {prefix_hash: kv_cache}
    
    def get(self, prefix_tokens):
        key = hash(tuple(prefix_tokens))
        return self.cache.get(key)
    
    def put(self, prefix_tokens, kv_cache):
        key = hash(tuple(prefix_tokens))
        self.cache[key] = kv_cache
```

**【你的优化任务】**：

1. **Radix Tree实现**
   ```python
                                          # TODO: 不是简单的hash，而是用Radix Tree
                                          # 支持部分匹配
                                          
                                          # 例如：
                                          # User 1: [1, 2, 3, 4, 5]
                                          # User 2: [1, 2, 3, 6, 7]
                                          # 共享前缀 [1, 2, 3]
                                          
                                          class RadixTree:
                                              def __init__(self):
                                                  self.root = Node()
                                              
                                              def insert(self, tokens, kv_cache):
                                                  node = self.root
                                                  for token in tokens:
                                                      if token not in node.children:
                                                          node.children[token] = Node()
                                                      node = node.children[token]
                                                  node.kv_cache = kv_cache
                                              
                                              def longest_prefix_match(self, tokens):
                                                  node = self.root
                                                  matched_tokens = []
                                                  matched_kv = []
                                                  
                                                  for token in tokens:
                                                      if token in node.children:
                                                          node = node.children[token]
                                                          matched_tokens.append(token)
                                                          if node.kv_cache is not None:
                                                              matched_kv.append(node.kv_cache)
                                                      else:
                                                          break
                                                  
                                                  return matched_tokens, matched_kv
   ```




2. **自动Eviction策略**
   ```python
                                          # TODO: 当显存不足时，淘汰哪些Prefix？
                                          # - LRU（最近最少使用）
                                          # - LFU（使用频率最低）
                                          # - Size-aware（大的prefix优先淘汰）
                                          
                                          class EvictionPolicy:
                                              def evict(self, cache, required_memory):
                                                  # 选择要淘汰的prefix
                                                  # 释放显存
                                                  # 更新cache
                                                  ...
   ```




3. **Benchmark：命中率分析**
   ```python
                                          # TODO: 在多轮对话场景下测试
                                          # - 1000个用户，共享相同的system prompt
                                          # - 计算命中率、显存节省、延迟降低
                                          
                                          # SGLang的数据：
                                          # - 命中率：80-90%
                                          # - TTFT（首token延迟）降低：5-10x
   ```


**验收标准**：

- ✅ 实现Radix Tree的Prefix Caching
- ✅ 多用户场景下，TTFT降低5倍以上
- ✅ 显存占用减少（共享prefix）
- ✅ 理解：为什么RadixAttention是"多租户推理"的关键

**【论文阅读】**：

- Efficient Memory Management for Large Language Model Serving with PagedAttention (vLLM)
- SGLang: Efficient Execution of Structured Language Model Programs
- 写博客：《RadixAttention：让多轮对话不重算的神技》

---

## 最终交付：世界顶级的简历

### 简历项目描述（终极版）

```markdown
**TitanInfer - 下一代LLM推理引擎**
（2024.12 - 2025.9，个人项目，GitHub 2000+ stars）

**架构创新**：
- 支持Dense Model (TinyLlama) 和 MoE (DeepSeek V3架构)
- 实现MLA (Multi-Head Latent Attention)，KV Cache压缩50%
- 支持FP8/INT4混合量化，显存占用减少75%
- 实现Speculative Decoding (Medusa)，延迟降低3倍
- 实现RadixAttention (Prefix Caching)，多轮对话TTFT降低5倍

**核心成就**：
- 用Rust实现PagedAttention内存管理器（Buddy Allocator），显存碎片率<4%
- 手写MLA的Paged Attention Kernel（Triton），支持FP8 KV Cache，HBM访问减少67%
- 手写FP8 GEMM Kernel，性能达到cuBLAS的82%，端到端推理提速1.6倍
- 实现MoE的Expert Batching和EP+TP混合并行，8卡吞吐量提升6.2倍，All-to-All通信开销<28%
- 实现Medusa架构的Tree Attention，推理延迟降低2.8倍（无需Draft Model）
- 实现RadixAttention（基于Radix Tree），多用户场景TTFT降低5.4倍

**性能对标**：
- Dense Model：达到vLLM的75%性能（吞吐量150 tokens/s vs 200 tokens/s）
- MoE架构：达到SGLang的70%性能（支持256专家）
- 长上下文：支持128K上下文，FP8 KV Cache显存占用减半

**技术博客**：8篇深度文章，总阅读量20万+
**开源影响**：被3个工业项目引用，10+个PR被合并到vLLM/FlashInfer
```



### 你能回答的"世界顶级"面试题

**DeepSeek面试官问**：> "DeepSeek V3在推理时，MLA架构相比Llama 3的GQA，在Kernel编写上最大的难点是什么？"**你的回答**：> "核心挑战有三个：> 1. **显存访问模式的变化**：MLA的KV Cache是compressed的（512维），需要在Kernel中动态解压。我在Triton中实现时，把解压矩阵放在Shared Memory，避免重复从HBM加载。> 2. **Matrix Absorption优化**：朴素实现会多一次矩阵乘法（解压），我通过预先合并解压矩阵和后续的O_proj，减少了一次GEMM。> 3. **FP8 KV Cache**：由于compressed KV更小，非常适合用FP8存储。但需要Per-Token的Scale，我在Block Table中额外存储了Scale tensor。>> 最终性能：相比GQA，MLA在128K上下文下显存节省48%，速度持平（解压开销被显存节省抵消）。"**OpenAI面试官问**：> "你如何优化一个MoE模型的All-to-All通信开销？"**你的回答**：> "我用了三个策略：> 1. **Expert Batching**：把选中同一个专家的token打包成batch，减少通信次数。我用Triton写了一个高效的Routing Kernel，提前构建expert_to_tokens映射。> 2. **通信与计算重叠**：在计算当前层的Expert时，异步地做下一层的All-to-All。PyTorch的async_op参数。> 3. **Hybrid Parallelism**：Expert间用EP（All-to-All），Expert内用TP（All-Reduce）。当GPU数远大于专家数时，这种混合策略效率更高。>> 在我的实现中，8卡256专家的DeepSeek V3架构，All-to-All占总时间的27%，低于SGLang的30%基准线。"---

## 执行计划与里程碑

### 时间线（8-9个月）

| 时间 | Phase | 关键里程碑 ||------|-------|-----------|| Week 0 | Module 0 | 掌握Nsight + Roofline || Week 1-3 | Phase 0 | TinyLlama + KV Cache || Week 4-6 | Phase 1 | Rust内存管理器 || Week 7-10 | Phase 2 | MLA架构 ⚡ || Week 11-13 | Phase 3 | FP8 KV Cache ⚡ || Week 14-16 | Phase 4 | 调度器 || Week 17-20 | Phase 5 | FP8 GEMM ⚡ || Week 21-24 | Phase 6 | MoE + EP ⚡ || Week 25-27 | Phase 7 | Speculative Decoding ⚡ || Week 28-30 | Phase 8 | RadixAttention ⚡ |

### 三个关键检查点

**检查点1（Week 10）**：MLA能力验证

- ✅ 实现MLA的完整forward
- ✅ KV Cache压缩50%
- ✅ 理解DeepSeek V3的核心创新
- **不达标不进入Phase 4**

**检查点2（Week 20）**：FP8能力验证

- ✅ FP8 GEMM达到cuBLAS的80%
- ✅ 端到端提速1.5倍
- ✅ 理解H100/B200的FP8 Tensor Core
- **不达标不进入Phase 6**

**检查点3（Week 30）**：架构师能力验证

- ✅ 支持MoE、MLA、FP8、Speculative、RadixAttention
- ✅ 性能达到vLLM/SGLang的70%
- ✅ 有8篇技术博客、对标报告
- ✅ 能回答所有"世界顶级"面试题
- **达标后直接投Principal级岗位**

---

## 为什么这个计划能让你拿到Principal级Offer？

### 对比评审者的建议

| 评审者要求 | 这个计划的实现 ||-----------|---------------|| MoE支持 | Phase 6：完整实现Expert Batching + EP + TP || MLA支持 | Phase 2：手写MLA Paged Attention Kernel || Speculative Decoding | Phase 7：实现Medusa + Tree Attention || FP8推理 | Phase 3（KV Cache）+ Phase 5（GEMM） |

### 你的竞争力

**普通候选人**：

- "我用过vLLM"
- "我知道PagedAttention"

**Senior候选人**：

- "我给vLLM贡献过代码"
- "我实现了FlashAttention"

**你（Principal候选人）**：

- "我实现了一个70%性能的vLLM，支持MoE、MLA、FP8"
- "我手写了DeepSeek V3的MLA Kernel，KV Cache压缩50%"
- "我的Medusa实现，延迟降低2.8倍"
- "这是我的8篇技术博客和对标报告"

---

## 立即开始

```bash
# 1. 安装工具链
pip install torch triton matplotlib pandas rust

# 2. 画Roofline图
python mountain/hpc/profiling/roofline.py

# 3. 下载模型
huggingface-cli download TinyLlama/TinyLlama-1.1B-Chat-v1.0

# 4. 开始Phase 0
cd mountain/hpc/llm
python inference_v0.py











```