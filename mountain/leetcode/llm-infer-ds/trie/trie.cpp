// ==============================================================================
// Trie (Prefix Tree) - 对应 LLM Inference 中的 Tokenizer, Prefix Caching
// ==============================================================================
//
// 【对应引擎模块】
//   - Tokenizer: 词表存储和高效查找（BPE 分词）
//   - Prefix Caching: 缓存共享前缀的 KV Cache（RadixAttention）
//
// 【学习重点】
//   1. 前缀匹配：多个序列共享相同前缀时的高效存储
//   2. 树形递归与指针操作：每个节点维护子节点的映射
//   3. 空间优化：相比哈希表，Trie 可以共享公共前缀
//
// 【实现要求】
//   - 实现 insert, search, startsWith (前缀查询) 操作
//   - 每个节点包含一个 map<char, TrieNode*> 或数组（26字母）
//   - 支持完整单词标记（is_end 标志）
//   - 可选：实现前缀匹配的批量查询
//
// 【练习目标】
//   - 理解 vLLM RadixAttention 中 Prefix Caching 的原理
//   - 掌握树形结构在前缀匹配场景的优势
//   - 模拟 Tokenizer 中词表的高效查找机制
// ==============================================================================

